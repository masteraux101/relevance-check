/**
 * Performance analysis and bottleneck diagnosis
 * 
 * Run: node tests/performance.test.js
 */

import * as tf from '@tensorflow/tfjs';  // Use pure JS version
import { DataProcessor } from '../src/utils/dataProcessor.js';

// Wait for TensorFlow initialization
await tf.ready();

console.log('');
console.log('â•'.repeat(60));
console.log('           Performance Analysis and Bottleneck Diagnosis');
console.log('â•'.repeat(60));
console.log('');

// Collect performance issues
const issues = [];

/**
 * Test 1: Detect tensor leaks
 */
async function detectTensorLeaks() {
    console.log('ğŸ” Detecting tensor memory leaks...\n');
    
    const iterations = 50;
    const tensorCounts = [];
    
    for (let i = 0; i < iterations; i++) {
        // Simulate common operations
        const input = tf.randomNormal([16, 60, 15]);
        const dense = tf.layers.dense({ units: 64 });
        const output = dense.apply(input);
        
        // Intentionally don't dispose, observe leaks
        if (i % 2 === 0) {
            input.dispose();
            output.dispose();
        }
        
        tensorCounts.push(tf.memory().numTensors);
        
        if (i % 10 === 0) {
            console.log(`  Iteration ${i}: tensor count ${tf.memory().numTensors}`);
        }
    }
    
    // Analyze leak trend
    const firstHalf = tensorCounts.slice(0, 25).reduce((a, b) => a + b, 0) / 25;
    const secondHalf = tensorCounts.slice(25).reduce((a, b) => a + b, 0) / 25;
    
    if (secondHalf > firstHalf * 1.5) {
        issues.push({
            type: 'TENSOR_LEAK',
            severity: 'HIGH',
            message: `Tensor leak detected: tensor count grew from average ${firstHalf.toFixed(0)} to ${secondHalf.toFixed(0)}`,
            suggestion: 'Ensure all intermediate tensors are disposed() or wrapped with tf.tidy()'
        });
    }
    
    // Cleanup
    tf.disposeVariables();
}

/**
 * Test 2: Detect synchronous blocking
 */
async function detectSyncBlocking() {
    console.log('\nğŸ” Detecting synchronous blocking operations...\n');
    
    const operations = [
        {
            name: 'arraySync()',
            test: () => {
                const tensor = tf.randomNormal([1000, 1000]);
                const start = performance.now();
                const arr = tensor.arraySync();  // Synchronous operation, will block
                const time = performance.now() - start;
                tensor.dispose();
                return time;
            }
        },
        {
            name: 'dataSync()',
            test: () => {
                const tensor = tf.randomNormal([1000, 1000]);
                const start = performance.now();
                const data = tensor.dataSync();  // Synchronous operation, will block
                const time = performance.now() - start;
                tensor.dispose();
                return time;
            }
        },
        {
            name: 'array() (async)',
            test: async () => {
                const tensor = tf.randomNormal([1000, 1000]);
                const start = performance.now();
                const arr = await tensor.array();  // Asynchronous operation, won't block
                const time = performance.now() - start;
                tensor.dispose();
                return time;
            }
        }
    ];
    
    for (const op of operations) {
        const time = await op.test();
        console.log(`  ${op.name}: ${time.toFixed(2)} ms`);
        
        if (time > 100 && !op.name.includes('async')) {
            issues.push({
                type: 'SYNC_BLOCKING',
                severity: 'MEDIUM',
                message: `Sync operation ${op.name} took ${time.toFixed(0)}ms`,
                suggestion: `Use async version like tensor.array() instead of tensor.arraySync()`
            });
        }
    }
}

/**
 * Test 3: Detect excessive recompilation
 */
async function detectRecompilation() {
    console.log('\nğŸ” Detecting model recompilation issues...\n');
    
    // Dynamic input shapes cause recompilation
    const shapes = [
        [1, 30, 15],
        [2, 30, 15],
        [4, 30, 15],
        [8, 30, 15],
        [16, 30, 15],
    ];
    
    const input = tf.input({ shape: [30, 15] });  // Fixed shape
    const output = tf.layers.dense({ units: 32 }).apply(input);
    const model = tf.model({ inputs: input, outputs: output });
    
    const times = [];
    for (const shape of shapes) {
        const testInput = tf.randomNormal(shape);
        const start = performance.now();
        const pred = model.predict(testInput);
        times.push(performance.now() - start);
        testInput.dispose();
        pred.dispose();
    }
    
    console.log('  Inference time for different batch sizes:');
    shapes.forEach((shape, i) => {
        console.log(`    [${shape.join(', ')}]: ${times[i].toFixed(2)} ms`);
    });
    
    // First call is usually slow (compilation), subsequent should be fast
    const firstTime = times[0];
    const avgSubsequent = times.slice(1).reduce((a, b) => a + b, 0) / (times.length - 1);
    
    if (times.some((t, i) => i > 0 && t > firstTime * 0.8)) {
        issues.push({
            type: 'RECOMPILATION',
            severity: 'HIGH',
            message: 'Possible repeated compilation detected, dynamic input shapes cause performance degradation',
            suggestion: 'Use fixed input shapes, avoid frequently changing batch size or sequence length'
        });
    }
    
    model.dispose();
}

/**
 * Test 4: Detect inefficient layer structures
 */
async function detectInefficientLayers() {
    console.log('\nğŸ” Detecting inefficient layer structures...\n');
    
    const batchSize = 16;
    const seqLen = 60;
    const features = 64;
    
    const layerTests = [
        {
            name: 'Dense (efficient)',
            create: () => {
                const input = tf.input({ shape: [seqLen, features] });
                const output = tf.layers.dense({ units: 64 }).apply(input);
                return tf.model({ inputs: input, outputs: output });
            }
        },
        {
            name: 'LSTM (medium)',
            create: () => {
                const input = tf.input({ shape: [seqLen, features] });
                const output = tf.layers.lstm({ units: 64, returnSequences: true }).apply(input);
                return tf.model({ inputs: input, outputs: output });
            }
        },
        {
            name: 'GRU (medium)',
            create: () => {
                const input = tf.input({ shape: [seqLen, features] });
                const output = tf.layers.gru({ units: 64, returnSequences: true }).apply(input);
                return tf.model({ inputs: input, outputs: output });
            }
        },
        {
            name: 'Multi-layer stacking',
            create: () => {
                const input = tf.input({ shape: [seqLen, features] });
                let x = tf.layers.dense({ units: 64 }).apply(input);
                x = tf.layers.lstm({ units: 32, returnSequences: true }).apply(x);
                x = tf.layers.gru({ units: 32, returnSequences: true }).apply(x);
                x = tf.layers.dense({ units: 64 }).apply(x);
                return tf.model({ inputs: input, outputs: x });
            }
        }
    ];
    
    for (const test of layerTests) {
        const model = test.create();
        const testInput = tf.randomNormal([batchSize, seqLen, features]);
        
        // Warmup
        model.predict(testInput).dispose();
        
        // Timing
        const times = [];
        for (let i = 0; i < 10; i++) {
            const start = performance.now();
            const pred = model.predict(testInput);
            pred.dispose();
            times.push(performance.now() - start);
        }
        
        const avgTime = times.reduce((a, b) => a + b, 0) / times.length;
        console.log(`  ${test.name}: ${avgTime.toFixed(2)} ms`);
        
        testInput.dispose();
        model.dispose();
    }
}

/**
 * Test 5: Detect data preprocessing bottleneck
 */
async function detectDataProcessingBottleneck() {
    console.log('\nğŸ” Detecting data preprocessing bottleneck...\n');
    
    const processor = new DataProcessor();
    
    // Test processing time for different data volumes
    const dataSizes = [100, 500, 1000, 2000, 5000];
    
    for (const size of dataSizes) {
        const start = performance.now();
        const data = processor.generateMockData('SPY', size);
        const genTime = performance.now() - start;
        
        const start2 = performance.now();
        const features = processor.extractFeatures(data);
        const extractTime = performance.now() - start2;
        
        const start3 = performance.now();
        const indicators = processor.calculateIndicators(data);
        const indicatorTime = performance.now() - start3;
        
        console.log(`  ${size} records:`);
        console.log(`    Generation: ${genTime.toFixed(2)} ms`);
        console.log(`    Feature extraction: ${extractTime.toFixed(2)} ms`);
        console.log(`    Indicator calculation: ${indicatorTime.toFixed(2)} ms`);
        
        // Detect super-linear growth
        if (size === 5000 && indicatorTime > 500) {
            issues.push({
                type: 'DATA_PROCESSING',
                severity: 'MEDIUM',
                message: `Data processing takes too long: ${size} records need ${indicatorTime.toFixed(0)}ms`,
                suggestion: 'Consider using vectorized operations or WebWorker for data processing'
            });
        }
    }
}

/**
 * Test 6: Detect WebGL backend issues
 */
async function detectBackendIssues() {
    console.log('\nğŸ” Detecting backend configuration...\n');
    
    console.log(`  Current backend: ${tf.getBackend()}`);
    console.log(`  Available backends: ${tf.engine().registryFactory ? Object.keys(tf.engine().registryFactory) : ['cpu', 'webgl']}`);
    
    const memInfo = tf.memory();
    console.log(`  Tensor count: ${memInfo.numTensors}`);
    console.log(`  Data buffers: ${memInfo.numDataBuffers}`);
    console.log(`  Allocated bytes: ${(memInfo.numBytes / 1024 / 1024).toFixed(2)} MB`);
    
    if (tf.getBackend() === 'cpu') {
        issues.push({
            type: 'BACKEND',
            severity: 'HIGH',
            message: 'Using CPU backend, performance will be slow',
            suggestion: 'Use WebGL backend in browsers, use tensorflow-node in Node.js'
        });
    }
}

/**
 * Test 7: Detect batch size issues
 */
async function detectBatchSizeIssues() {
    console.log('\nğŸ” Detecting batch size performance...\n');
    
    const input = tf.input({ shape: [60, 15] });
    let x = tf.layers.dense({ units: 32 }).apply(input);
    x = tf.layers.lstm({ units: 32, returnSequences: false }).apply(x);
    const output = tf.layers.dense({ units: 20 }).apply(x);
    const model = tf.model({ inputs: input, outputs: output });
    
    const batchSizes = [1, 2, 4, 8, 16, 32, 64];
    const results = [];
    
    for (const bs of batchSizes) {
        const testInput = tf.randomNormal([bs, 60, 15]);
        
        // é¢„çƒ­
        model.predict(testInput).dispose();
        
        const times = [];
        for (let i = 0; i < 5; i++) {
            const start = performance.now();
            const pred = model.predict(testInput);
            pred.dispose();
            times.push(performance.now() - start);
        }
        
        const avgTime = times.reduce((a, b) => a + b, 0) / times.length;
        const perSample = avgTime / bs;
        
        results.push({ bs, avgTime, perSample });
        console.log(`  æ‰¹æ¬¡ ${bs.toString().padStart(2)}: æ€» ${avgTime.toFixed(2).padStart(8)} ms, æ¯æ ·æœ¬ ${perSample.toFixed(2).padStart(6)} ms`);
        
        testInput.dispose();
    }
    
    // æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    const optimal = results.reduce((min, r) => r.perSample < min.perSample ? r : min);
    console.log(`\n  ğŸ“Š æœ€ä¼˜æ‰¹æ¬¡å¤§å°: ${optimal.bs} (æ¯æ ·æœ¬ ${optimal.perSample.toFixed(2)} ms)`);
    
    if (optimal.bs > 1) {
        issues.push({
            type: 'BATCH_SIZE',
            severity: 'LOW',
            message: `å»ºè®®ä½¿ç”¨æ‰¹æ¬¡å¤§å° ${optimal.bs} ä»¥è·å¾—æœ€ä½³æ€§èƒ½`,
            suggestion: 'æ‰¹é‡å¤„ç†æ•°æ®è€Œä¸æ˜¯é€ä¸ªå¤„ç†'
        });
    }
    
    model.dispose();
}

/**
 * æµ‹è¯•8: æ£€æµ‹è®­ç»ƒå¾ªç¯æ•ˆç‡
 */
async function detectTrainingEfficiency() {
    console.log('\nğŸ” æ£€æµ‹è®­ç»ƒå¾ªç¯æ•ˆç‡...\n');
    
    const input = tf.input({ shape: [30, 10] });
    let x = tf.layers.dense({ units: 16 }).apply(input);
    x = tf.layers.lstm({ units: 16, returnSequences: false }).apply(x);
    const output = tf.layers.dense({ units: 5 }).apply(x);
    const model = tf.model({ inputs: input, outputs: output });
    
    model.compile({ optimizer: 'adam', loss: 'meanSquaredError' });
    
    const trainX = tf.randomNormal([100, 30, 10]);
    const trainY = tf.randomNormal([100, 5]);
    
    // æµ‹è¯•ä¸åŒè®­ç»ƒé…ç½®
    const configs = [
        { batchSize: 8, epochs: 5, desc: 'å°æ‰¹æ¬¡' },
        { batchSize: 32, epochs: 5, desc: 'ä¸­æ‰¹æ¬¡' },
        { batchSize: 64, epochs: 5, desc: 'å¤§æ‰¹æ¬¡' }
    ];
    
    for (const config of configs) {
        const start = performance.now();
        await model.fit(trainX, trainY, {
            epochs: config.epochs,
            batchSize: config.batchSize,
            verbose: 0
        });
        const time = performance.now() - start;
        
        console.log(`  ${config.desc} (${config.batchSize}): ${time.toFixed(2)} ms / ${config.epochs} epochs`);
    }
    
    trainX.dispose();
    trainY.dispose();
    model.dispose();
}

/**
 * è¾“å‡ºè¯Šæ–­æŠ¥å‘Š
 */
function printReport() {
    console.log('');
    console.log('â•'.repeat(60));
    console.log('                    è¯Šæ–­æŠ¥å‘Š');
    console.log('â•'.repeat(60));
    console.log('');
    
    if (issues.length === 0) {
        console.log('âœ… æœªå‘ç°æ˜æ˜¾æ€§èƒ½é—®é¢˜\n');
        return;
    }
    
    // æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
    const severityOrder = { 'HIGH': 0, 'MEDIUM': 1, 'LOW': 2 };
    issues.sort((a, b) => severityOrder[a.severity] - severityOrder[b.severity]);
    
    console.log(`å‘ç° ${issues.length} ä¸ªæ½œåœ¨é—®é¢˜:\n`);
    
    for (const issue of issues) {
        const icon = {
            'HIGH': 'ğŸ”´',
            'MEDIUM': 'ğŸŸ¡',
            'LOW': 'ğŸŸ¢'
        }[issue.severity];
        
        console.log(`${icon} [${issue.severity}] ${issue.type}`);
        console.log(`   é—®é¢˜: ${issue.message}`);
        console.log(`   å»ºè®®: ${issue.suggestion}`);
        console.log('');
    }
    
    // æ€§èƒ½ä¼˜åŒ–å»ºè®®
    console.log('â•'.repeat(60));
    console.log('                  æ€§èƒ½ä¼˜åŒ–å»ºè®®');
    console.log('â•'.repeat(60));
    console.log('');
    console.log('1. ä½¿ç”¨ tf.tidy() åŒ…è£…æ‰€æœ‰å¼ é‡æ“ä½œï¼Œé¿å…å†…å­˜æ³„æ¼');
    console.log('2. ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥æ“ä½œ (array/data) è€ŒéåŒæ­¥æ“ä½œ (arraySync/dataSync)');
    console.log('3. å›ºå®šæ¨¡å‹è¾“å…¥å½¢çŠ¶ï¼Œé¿å…åŠ¨æ€é‡æ–°ç¼–è¯‘');
    console.log('4. ä½¿ç”¨åˆé€‚çš„æ‰¹æ¬¡å¤§å°è¿›è¡Œæ¨ç†å’Œè®­ç»ƒ');
    console.log('5. è€ƒè™‘ä½¿ç”¨ WebWorker å¤„ç†æ•°æ®é¢„å¤„ç†');
    console.log('6. åœ¨æµè§ˆå™¨ä¸­ç¡®ä¿ WebGL åç«¯å¯ç”¨');
    console.log('7. å‡å°‘æ¨¡å‹å±‚æ•°æˆ–éšè—ç»´åº¦ä»¥æå‡é€Ÿåº¦');
    console.log('');
}

// è¿è¡Œæ‰€æœ‰è¯Šæ–­
async function runDiagnostics() {
    try {
        await detectBackendIssues();
        await detectTensorLeaks();
        await detectSyncBlocking();
        await detectRecompilation();
        await detectInefficientLayers();
        await detectDataProcessingBottleneck();
        await detectBatchSizeIssues();
        await detectTrainingEfficiency();
        
        printReport();
    } catch (error) {
        console.error('è¯Šæ–­è¿‡ç¨‹å‡ºé”™:', error);
    }
}

runDiagnostics();
